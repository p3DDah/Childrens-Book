{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81693ecdc88dbd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:30:42.770317300Z",
     "start_time": "2023-11-19T07:30:30.268649300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#import gradio as gr\n",
    "\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, AutoConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(torch.cuda.is_available())\n",
    "num_chapters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eff7472620e5a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:32:27.071609300Z",
     "start_time": "2023-11-19T07:31:39.691055100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin C:\\Users\\phili\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121_nocublaslt.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Users\\phili\\miniconda3\\envs\\ChildrensBook\\bin\\cudart64_12.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary C:\\Users\\phili\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121_nocublaslt.dll...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b91d1799e1c34e63b537a8c425a4d9f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#MODEL_NAME = \"NousResearch/Yarn-Mistral-7b-128k\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    #use_flash_attention = True,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# name = 'mosaicml/mpt-7b-storywriter'\n",
    "# \n",
    "# config = AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "# # config.attn_config['attn_impl'] = 'triton'\n",
    "# config.init_device = 'cuda:0' \n",
    "# config.max_seq_len = 83968 # (input + output) tokens can now be up to 83968\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "# \n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#   name,\n",
    "#   config=config,\n",
    "#   torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
    "#   trust_remote_code=True,\n",
    "#   device_map=\"auto\",\n",
    "#   quantization_config=quantization_config\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11622b5170b74735"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c24c84de4e247c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:17.877034800Z",
     "start_time": "2023-11-19T07:34:17.127658200Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['model', 'tokenizer'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m generation_config\u001B[38;5;241m.\u001B[39mtemperature \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.75\u001B[39m\n\u001B[0;32m      7\u001B[0m generation_config\u001B[38;5;241m.\u001B[39mrepetition_penalty \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m----> 9\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m pipeline(\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     12\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[0;32m     13\u001B[0m     return_full_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     14\u001B[0m     generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m     15\u001B[0m )\n\u001B[0;32m     16\u001B[0m pipeline\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39meos_token_id\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:208\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[1;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    168\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03m    Complete the prompt(s) given as inputs.\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m          ids of the generated text.\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(text_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[0;32m   1134\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1137\u001B[0m         )\n\u001B[0;32m   1138\u001B[0m     )\n\u001B[0;32m   1139\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m   1146\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[1;32m-> 1147\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1148\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[1;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[0;32m   1044\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[0;32m   1045\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1046\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1047\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:271\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[1;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[0;32m    268\u001B[0m         generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_length\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m prefix_length\n\u001B[0;32m    270\u001B[0m \u001B[38;5;66;03m# BS x SL\u001B[39;00m\n\u001B[1;32m--> 271\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgenerate(input_ids\u001B[38;5;241m=\u001B[39minput_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs)\n\u001B[0;32m    272\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\generation\\utils.py:1565\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1563\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# All unused kwargs must be model kwargs\u001B[39;00m\n\u001B[0;32m   1564\u001B[0m generation_config\u001B[38;5;241m.\u001B[39mvalidate()\n\u001B[1;32m-> 1565\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_model_kwargs(model_kwargs\u001B[38;5;241m.\u001B[39mcopy())\n\u001B[0;32m   1567\u001B[0m \u001B[38;5;66;03m# 2. Set generation parameters if not already defined\u001B[39;00m\n\u001B[0;32m   1568\u001B[0m logits_processor \u001B[38;5;241m=\u001B[39m logits_processor \u001B[38;5;28;01mif\u001B[39;00m logits_processor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m LogitsProcessorList()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ChildrensBook\\Lib\\site-packages\\transformers\\generation\\utils.py:1342\u001B[0m, in \u001B[0;36mGenerationMixin._validate_model_kwargs\u001B[1;34m(self, model_kwargs)\u001B[0m\n\u001B[0;32m   1339\u001B[0m         unused_model_args\u001B[38;5;241m.\u001B[39mappend(key)\n\u001B[0;32m   1341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m unused_model_args:\n\u001B[1;32m-> 1342\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1343\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following `model_kwargs` are not used by the model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00munused_model_args\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (note: typos in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1344\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m generate arguments will also show up in this list)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1345\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The following `model_kwargs` are not used by the model: ['model', 'tokenizer'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 2000\n",
    "generation_config.max_length = 10000\n",
    "generation_config.top_p = 1\n",
    "generation_config.do_sample = True\n",
    "generation_config.temperature = 0.75\n",
    "generation_config.repetition_penalty = 1\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "pipeline.model.config.pad_token_id = pipeline.model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef1910676d2f256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:19.168062100Z",
     "start_time": "2023-11-19T07:34:19.150798200Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96ad765b0d5d387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:20.225752700Z",
     "start_time": "2023-11-19T07:34:20.225223400Z"
    }
   },
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"<s>[INST]\n",
    "Objective: Create a 4-chapter children's book set in a magical Korean world with dragons. Each 500-word chapter features a unique, open-ended adventure.\n",
    "AI's Role:\n",
    "Craft engaging, informative stories for young readers.\n",
    "Use a friendly tone, rich descriptions, diverse characters.\n",
    "Focus on positive themes like friendship and learning.\n",
    "Mix in humor and educational elements.\n",
    "Build resilience and empathy through challenges.\n",
    "End stories with hope or lessons, avoiding negativity.\n",
    "Process: Write one detailed chapter at a time, awaiting user prompts for subsequent chapters.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "USER: {input}\n",
    "AI: [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642d1ee3c4c39aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:22.426648400Z",
     "start_time": "2023-11-19T07:34:22.410478500Z"
    }
   },
   "outputs": [],
   "source": [
    "longchat_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"history\"], template=longchat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20dbf513ccc839a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:23.026583700Z",
     "start_time": "2023-11-19T07:34:23.010452200Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"AI\", human_prefix=\"USER\"),\n",
    "    prompt=longchat_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ce62d8346cb6fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:24.195401200Z",
     "start_time": "2023-11-19T07:34:24.163776700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "# \n",
    "# conversation_buf = ConversationChain(\n",
    "# \tllm=llm,\n",
    "# \tmemory=ConversationSummaryMemory(llm=llm, ai_prefix=\"AI\", human_prefix=\"USER\"),\n",
    "#   prompt=longchat_prompt_template\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da5e18709d94110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:24.673689500Z",
     "start_time": "2023-11-19T07:34:24.668375600Z"
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# \n",
    "# conversation_buf = ConversationChain(\n",
    "#     llm=llm, \n",
    "#     prompt=longchat_prompt_template,\n",
    "#     memory=ConversationSummaryBufferMemory(\n",
    "#         llm=llm,\n",
    "#         max_token_limit=1300,\n",
    "#         ai_prefix=\"AI\", \n",
    "#         human_prefix=\"USER\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc39271ce40c00ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:24.857956200Z",
     "start_time": "2023-11-19T07:34:24.851867500Z"
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationKGMemory\n",
    "# \n",
    "# conversation_buf = ConversationChain(\n",
    "# \tllm=llm,\n",
    "# \tmemory=ConversationKGMemory(llm=llm),\n",
    "#   prompt=longchat_prompt_template\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T07:34:26.197745400Z",
     "start_time": "2023-11-19T07:34:26.194728800Z"
    }
   },
   "id": "f990d58146c62194"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c45657100ddae421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T07:39:04.395813Z",
     "start_time": "2023-11-19T07:35:36.800076100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter 1: The Dragon Kingdom Discovery\n",
      "\n",
      "Title: \"The Secret Garden of Dragons\"\n",
      "\n",
      "Once upon a time, in a far-off land filled with lush greenery and sparkling rivers, there lived a young boy named Jae. Jae lived with his family in a small village surrounded by forests and mountains. Despite living near such breathtaking nature, Jae felt like he was missing out on the adventure of a lifetime.\n",
      "\n",
      "One sunny morning, while exploring the forest near his home, Jae stumbled upon a hidden pathway. Curiosity piqued, he followed the winding trail deeper into the woods. As he walked, he noticed the trees becoming tall and twisted, the air growing thicker and cooler. Suddenly, the sound of rushing water caught his attention.\n",
      "\n",
      "Following the sound, Jae came across a beautiful waterfall cascading down into a crystal-clear pool. Surrounding the pool was a lush garden overflowing with colorful flowers and exotic plants. Jae was amazed by the beauty of the garden and decided to explore further.\n",
      "\n",
      "As he walked through the garden, Jae noticed something unusual. Everywhere he looked, there were small, intricate sculptures of dragons. Jae had never seen anything like them before. They were so detailed and lifelike, it was like they were real!\n",
      "\n",
      "Suddenly, Jae heard a loud rumble in the distance. The ground started shaking, and the trees started swaying. Jae knew something was coming, and he quickly ran towards the source of the noise.\n",
      "\n",
      "As Jae approached the source of the rumble, he saw a magnificent dragon emerging from the forest. The dragon had scales the color of the sky, eyes that glowed like the sun, and wings as wide as the sky. Jae was in awe of the dragon's beauty and power.\n",
      "\n",
      "The dragon looked down at Jae and spoke in a deep, booming voice, \"Welcome to the Secret Garden of Dragons. I am the guardian of this place, and I have been waiting for someone like you to come and discover its wonders.\"\n",
      "\n",
      "Jae couldn't believe what he was hearing. Was this place real? Was he really talking to a dragon? Jae felt like he was living in a fairy tale.\n",
      "\n",
      "As Jae spent the day exploring the Secret Garden of Dragons, he learned so much about the magic of the world around him. He met other dragons, each with their own unique abilities and personalities. Jae even learned how to control the elements, just like the dragons themselves.\n",
      "\n",
      "As the sun began to set, Jae knew that it was time for him to return home. But before he left, the dragon gave him a small, golden key. \"This key will unlock the magic of the world around you,\" the dragon said. \"Use it wisely, and always remember the lessons you've learned here.\"\n",
      "\n",
      "Jae thanked the dragon for the incredible adventure and promised to use the key wisely. As he walked back towards his village, he felt like he was walking on top of the world. He knew that he had discovered something truly magical, and he couldn't wait to explore the world around him with the key in his pocket.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf(\"Write now only the first chapter and give it a title!\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c8ee1e887a32b",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-19T07:39:04.403321400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf(f\"[INST] Give me 4 decision possibilities of how the story could continue! Write no more than two sentences. [/INST]\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e37fa419ded6d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_dec = 4  # number of decisions\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "if num_dec > len(letters):\n",
    "    num_dec = len(letters)\n",
    "for i in range(num_chapters-2):\n",
    "    decision = random.choice(letters[:num_dec])\n",
    "    print(f\"Decision: {decision}\")\n",
    "    print(conversation_buf(f\" Continue the story with decision {decision}. Create only the next chapter now! Keep in mind to write at least 500 words! [/INST]\")[\"response\"])\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "    print(conversation_buf(f\"[INST] Give me four decision possibilities of how the story could continue. Write no more than two sentences. [/INST]\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb08c45ab9a9be3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "decision = \"c\"\n",
    "print(conversation_buf(f\"[INST] Continue the story with decision {decision}. Create only the next and last chapter now! Keep in mind to write at least 500 words! Find a happy or sad ending based on the previous decisions. [/INST]\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845ba48eb57e1ab",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a7d393b2f2f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
