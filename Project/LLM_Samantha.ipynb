{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x7f960e707110>\n",
      "NVIDIA A100-SXM4-80GB\n",
      "12.1\n",
      "GPU 0:\n",
      "  Total Memory: 81092.56 MB\n",
      "  Allocated Memory: 14332.65 MB\n",
      "  Cached Memory: 28800.00 MB\n",
      "--------------------------------------------------\n",
      "tensor([1., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs available\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}:\")\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "\n",
    "        # Total memory\n",
    "        total_memory = gpu_properties.total_memory / 1024**2  # Convert to MB\n",
    "        print(f\"  Total Memory: {total_memory:.2f} MB\")\n",
    "\n",
    "        # Current memory usage\n",
    "        allocated_memory = torch.cuda.memory_allocated(i) / 1024**2  # Convert to MB\n",
    "        print(f\"  Allocated Memory: {allocated_memory:.2f} MB\")\n",
    "\n",
    "        # Cached memory\n",
    "        cached_memory = torch.cuda.memory_reserved(i) / 1024**2  # Convert to MB\n",
    "        print(f\"  Cached Memory: {cached_memory:.2f} MB\")\n",
    "\n",
    "        print(\"--------------------------------------------------\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "print(torch.tensor([1.0, 2.0], device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  1 17:00:11 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB           On | 00000000:46:00.0 Off |                    0 |\n",
      "| N/A   29C    P0               66W / 400W|  30135MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1053981      C   ...lipp/anaconda3/envs/book/bin/python    30132MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81693ecdc88dbd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:49:17.526365300Z",
     "start_time": "2023-11-19T12:49:07.610460100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#import gradio as gr\n",
    "\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline, AutoConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_CHAPTERS = 10\n",
    "MAIN_NAME = \"Klaus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eff7472620e5a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:21.995539600Z",
     "start_time": "2023-11-19T12:49:17.526365300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc133f1ea124d3ab2a00622acc8e83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da1e275e24243d88e49073940da5e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aa52664bc84a0d9071e01cdeba6307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a592977684745e3b80d530685edc319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5082b2a064024d45a714b919399b5570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_mpt.py:   0%|          | 0.00/11.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- configuration_mpt.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70582c7f87fe41dda6e39abcfada8c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_mpt.py:   0%|          | 0.00/20.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2438e826fe7a4b6c88c9ff998e5d25d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "blocks.py:   0%|          | 0.00/2.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7871c54cd7de46c7be2217ac3feb06fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ffn.py:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c829a109d95c413b84134719920a4e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fc.py:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- fc.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- ffn.py\n",
      "- fc.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaf51ad3355444896e772fd8fefee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "attention.py:   0%|          | 0.00/21.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95422a581131433691da8f508f5ab5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1597e33193b40d19155ee7744a115b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norm.py:   0%|          | 0.00/3.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- attention.py\n",
      "- flash_attn_triton.py\n",
      "- norm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- blocks.py\n",
      "- ffn.py\n",
      "- attention.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05956fea29c44d587d6ad4721652e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "param_init_fns.py:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- param_init_fns.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d07cd5f1838464fae1ffe07aa2b9974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapt_tokenizer.py:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3930a6a2a5934537b989e8effa7b9fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "meta_init_context.py:   0%|          | 0.00/3.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- meta_init_context.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e07be645dc4175a88f26fd84dc6267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "custom_embedding.py:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- custom_embedding.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e837c87e41411fb4a8a96a17d810ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hf_prefixlm_converter.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n",
      "- modeling_mpt.py\n",
      "- blocks.py\n",
      "- param_init_fns.py\n",
      "- adapt_tokenizer.py\n",
      "- meta_init_context.py\n",
      "- custom_embedding.py\n",
      "- hf_prefixlm_converter.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a50c2fb2f344708b23fb46ee61a657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3a86cf5ec346d187ce28c807be41f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff2f0f9029e437c952cc56a387af849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba852def8754f8a90eb0a09efbb2c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0a398a9ceb42edb7ce39d596f20f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2400e5c220724d6e92561dbba5ce7ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#MODEL_NAME = \"FlexingD/yarn-mistral-7B-64k-instruct-alpaca-cleaned-origin\"\n",
    "#MODEL_NAME = \"NousResearch/Yarn-Mistral-7b-128k\"\n",
    "MODEL_NAME = 'mosaicml/mpt-7b'\n",
    "#MODEL_NAME = 'Intel/neural-chat-7b-v3-1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    use_flash_attention_2=False,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c24c84de4e247c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:22.259325800Z",
     "start_time": "2023-11-19T12:50:22.011652Z"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1000\n",
    "#generation_config.min_new_tokens = -1\n",
    "#generation_config.max_length = 10000\n",
    "#generation_config.top_p = 0.9\n",
    "#generation_config.top_k = 50\n",
    "#generation_config.do_sample = True\n",
    "#generation_config.temperature = 0.7\n",
    "generation_config.repetition_penalty = 1.2\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "pipeline.model.config.pad_token_id = pipeline.model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cef1910676d2f256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:22.275185Z",
     "start_time": "2023-11-19T12:50:22.259325800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a96ad765b0d5d387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:22.308622300Z",
     "start_time": "2023-11-19T12:50:22.275185Z"
    }
   },
   "outputs": [],
   "source": [
    "longchat_template = f\"\"\"\n",
    "<s>[INST] Objective: Create a {NUM_CHAPTERS}-chapter children's book set in a magical Korean world with dragons. Each 1000-word chapter features a unique, open-ended adventure.\n",
    "Assistants Role:\n",
    "Craft engaging, informative stories for young readers.\n",
    "Use a friendly tone, rich descriptions, diverse characters.\n",
    "Focus on positive themes like friendship and learning.\n",
    "Mix in humor and educational elements.\n",
    "Build resilience and empathy through challenges.\n",
    "End stories with hope or lessons, avoiding negativity.\n",
    "Process: Write one detailed chapter at a time, awaiting user prompts for subsequent chapters.\n",
    "\n",
    "Prologue: The Quest for Harmony in Korea's Enchanted Realms\n",
    "\n",
    "In a realm where ancient mystique and modern marvels intertwine, an epic saga awaits to be told. Here, in the heart of Korea's enchanted lands, a world brimming with magic and mystery beckons. A world where dragons trace majestic arcs across the sky and mythical beings whisper the secrets of the ages.\n",
    "In this realm of wonder, two heroes are predestined to unite: {MAIN_NAME}, the resplendent phoenix from the soaring cliffs of Pohang, a creature of fire and lore, and Nubzuki, the inquisitive platfish from the tranquil waters of Daejeon, wise and whimsical.\n",
    "Their journey, a tapestry of peril and enlightenment, will sweep them through realms where ancient trees murmur forgotten tales, across vast meadows echoing with celestial music, and into vibrant cities where history and future meld in perfect harmony.\n",
    "This odyssey, however, is far from a mere escapade; it's a quest of discovery, resilience, and kinship. Through trials and tribulations, {MAIN_NAME} and Nubzuki will delve deep into the heart of Korean culture, encountering its rich traditions, arts, and ancient wisdoms, each step a lesson in unity and strength.\n",
    "But lurking in the shadows is a darkness, a force that seeks to unbalance this world of wonder. Our heroes, {MAIN_NAME} and Nubzuki, must stand as one against this burgeoning chaos. In their quest, they will learn that true might blossoms from togetherness and that courage can be found in the most unexpected places.\n",
    "So, brace yourselves, dear readers, for an adventure beyond compare. Soar with {MAIN_NAME} into realms uncharted, dive with Nubzuki into the depths of the extraordinary. Together, we will traverse a world where Korean lore comes alive, where each twist and turn is a doorway to the extraordinary.\n",
    "Prepare to embark on \"The Quest for Harmony in Korea's Enchanted Realms,\" a narrative pulsating with magic, brimming with lessons, and shimmering with the light of enduring hope.\n",
    "\"\"\"\n",
    "longchat_template.format(MAIN_NAME = MAIN_NAME, NUM_CHAPTERS = NUM_CHAPTERS)\n",
    "longchat_template += \"\"\"\n",
    "Current conversation:\n",
    "{history}\n",
    "User: \n",
    "{input}\n",
    "Assistant: [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = f\"\"\"### System: \n",
    "Objective: Create a {NUM_CHAPTERS}-chapter children's book set in a magical Korean world with dragons. Each 1000-word chapter features a unique, open-ended adventure.\n",
    "AI's Role:\n",
    "Craft engaging, informative stories for young readers.\n",
    "Use a friendly tone, rich descriptions, diverse characters.\n",
    "Focus on positive themes like friendship and learning.\n",
    "Mix in humor and educational elements.\n",
    "Build resilience and empathy through challenges.\n",
    "End stories with hope or lessons, avoiding negativity.\n",
    "Process: Write one detailed chapter at a time, awaiting user prompts for subsequent chapters.\n",
    "\n",
    "Prologue: The Quest for Harmony in Korea's Enchanted Realms\n",
    "\n",
    "In a realm where ancient mystique and modern marvels intertwine, an epic saga awaits to be told. Here, in the heart of Korea's enchanted lands, a world brimming with magic and mystery beckons. A world where dragons trace majestic arcs across the sky and mythical beings whisper the secrets of the ages. \n",
    "In this realm of wonder, two heroes are predestined to unite: {MAIN_NAME}, the resplendent phoenix from the soaring cliffs of Pohang, a creature of fire and lore, and Nubzuki, the inquisitive platfish from the tranquil waters of Daejeon, wise and whimsical.\n",
    "Their journey, a tapestry of peril and enlightenment, will sweep them through realms where ancient trees murmur forgotten tales, across vast meadows echoing with celestial music, and into vibrant cities where history and future meld in perfect harmony.\n",
    "This odyssey, however, is far from a mere escapade; it's a quest of discovery, resilience, and kinship. Through trials and tribulations, {MAIN_NAME} and Nubzuki will delve deep into the heart of Korean culture, encountering its rich traditions, arts, and ancient wisdoms, each step a lesson in unity and strength.\n",
    "But lurking in the shadows is a darkness, a force that seeks to unbalance this world of wonder. Our heroes, {MAIN_NAME} and Nubzuki, must stand as one against this burgeoning chaos. In their quest, they will learn that true might blossoms from togetherness and that courage can be found in the most unexpected places.\n",
    "So, brace yourselves, dear readers, for an adventure beyond compare. Soar with {MAIN_NAME} into realms uncharted, dive with Nubzuki into the depths of the extraordinary. Together, we will traverse a world where Korean lore comes alive, where each twist and turn is a doorway to the extraordinary.\n",
    "Prepare to embark on \"The Quest for Harmony in Korea's Enchanted Realms,\" a narrative pulsating with magic, brimming with lessons, and shimmering with the light of enduring hope.\n",
    "\"\"\"\n",
    "longchat_template.format(MAIN_NAME = MAIN_NAME, NUM_CHAPTERS = NUM_CHAPTERS)\n",
    "longchat_template += \"\"\"\n",
    "{history}\n",
    "### User: \n",
    "{input}\n",
    "### Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642d1ee3c4c39aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:22.333111700Z",
     "start_time": "2023-11-19T12:50:22.294631600Z"
    }
   },
   "outputs": [],
   "source": [
    "longchat_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=longchat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20dbf513ccc839a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:22.355906200Z",
     "start_time": "2023-11-19T12:50:22.306641500Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    "    prompt=longchat_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc39271ce40c00ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:22.355906200Z",
     "start_time": "2023-11-19T12:50:22.340303Z"
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationKGMemory\n",
    "# \n",
    "# conversation_buf = ConversationChain(\n",
    "# \tllm=llm,\n",
    "# \tmemory=ConversationKGMemory(llm=llm, ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    "#   prompt=longchat_prompt_template\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c45657100ddae421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:27.760905400Z",
     "start_time": "2023-11-19T12:50:22.341854900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am ready when you write your next prompt :)\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf(\"Write now only the first chapter!\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "248c8ee1e887a32b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T12:50:27.773891800Z",
     "start_time": "2023-11-19T12:50:27.773891800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have created four options based upon what has been written before (see below). You may choose any option by writing '1', etc., but please do not change anything else about my text until after all choices were made - otherwise there would no longer remain something which was already decided earlier...\n",
      "\n",
      "    Klaus & his friends go back home \n",
      "        * 1) They find out more information regarding why people don't want him around anymore? What does he discover during these investigations? How did things get better afterwards...  (write some details here!) \n",
      "\n",
      "            2a ) He finds new allies who help fight evil forces / monsters/ whatever!!!   What happens then??\n",
      "\n",
      "\n",
      "                3b : His old enemies come up again!! Why?? And HOW DOES HE FIGHT THEM AGAIN???\n",
      "\n",
      "\n",
      "\n",
      "                    4c): There seems also another way than fighting.. maybe diplomacy works too..?! But WHY DIDN'T THEY TRUST HIM IN THE FIRST PLACE????!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        5:) Something completely different happened instead.... WHAT WAS IT THEN!!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "968e37fa419ded6d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T12:50:27.773891800Z"
    }
   },
   "outputs": [],
   "source": [
    "#import random\n",
    "#random.seed()\n",
    "#num_dec = 4  # number of decisions\n",
    "#for i in range(NUM_CHAPTERS - 2):\n",
    "#    decision = random.randint(1, num_dec)\n",
    "#    print(f\"Decision: {decision}\")\n",
    "#    print(conversation_buf(f\" Write now the next chapter with using decision {decision}.\")[\"response\"])\n",
    "#    print()\n",
    "#    print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wrote down three possible continuations according... ;) Please select ONE OF THESE THREE OPTIONS BY WRITING YOUR CHOICE NUMBER HERE BELOW AND START TO DESCRIBE NOW WHERE YOU WOULD LIKE THIS STORY GO FROM THERE ON OUT.... :-)!!!!!!!!!\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/philipp/Childrens-Book/Project/LLM_Samantha.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.223.25.21/home/philipp/Childrens-Book/Project/LLM_Samantha.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(conversation_buf(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrite now the next chapter of \u001b[39m\u001b[39m{\u001b[39;00mNUM_CHAPTERS\u001b[39m}\u001b[39;00m\u001b[39m with using decision \u001b[39m\u001b[39m{\u001b[39;00mdecision\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.223.25.21/home/philipp/Childrens-Book/Project/LLM_Samantha.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B141.223.25.21/home/philipp/Childrens-Book/Project/LLM_Samantha.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(conversation_buf(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCreate now 4 decisions how the story could continue.\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain/chains/llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    116\u001b[0m         prompts,\n\u001b[1;32m    117\u001b[0m         stop,\n\u001b[1;32m    118\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    119\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[1;32m    123\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[1;32m    124\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain_core/language_models/llms.py:506\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    499\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    500\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    504\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    505\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain_core/language_models/llms.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m         )\n\u001b[1;32m    643\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    644\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    645\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain_core/language_models/llms.py:543\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    542\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    544\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    545\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain_core/language_models/llms.py:530\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    522\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    527\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    528\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 530\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    531\u001b[0m                 prompts,\n\u001b[1;32m    532\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    533\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    534\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    535\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    536\u001b[0m             )\n\u001b[1;32m    537\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    538\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    539\u001b[0m         )\n\u001b[1;32m    540\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    541\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/langchain/llms/huggingface_pipeline.py:203\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m batch_prompts \u001b[39m=\u001b[39m prompts[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n\u001b[1;32m    202\u001b[0m \u001b[39m# Process batch of prompts\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m responses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipeline(batch_prompts)\n\u001b[1;32m    205\u001b[0m \u001b[39m# Process each response in the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mfor\u001b[39;00m j, response \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/generation/utils.py:1673\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1657\u001b[0m         input_ids,\n\u001b[1;32m   1658\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1670\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1674\u001b[0m         input_ids,\n\u001b[1;32m   1675\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1676\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1677\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1678\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1679\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1680\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1681\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1682\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1683\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1684\u001b[0m     )\n\u001b[1;32m   1686\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1687\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/generation/utils.py:2582\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m         this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2581\u001b[0m \u001b[39m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m \u001b[39mif\u001b[39;00m stopping_criteria(input_ids, scores):\n\u001b[1;32m   2583\u001b[0m     this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2585\u001b[0m \u001b[39mif\u001b[39;00m this_peer_finished \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:130\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStoppingCriteriaList\u001b[39;00m(\u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m     \u001b[39m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    131\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mLongTensor, scores: torch\u001b[39m.\u001b[39mFloatTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(criteria(input_ids, scores) \u001b[39mfor\u001b[39;00m criteria \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n\u001b[1;32m    134\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mmax_length\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed()\n",
    "num_dec = 4  # number of decisions\n",
    "\n",
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = random.randint(1, num_dec)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(conversation_buf(f\"Write now the next chapter of {NUM_CHAPTERS} with using decision {decision}.\")[\"response\"])\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "print(conversation_buf(f\"Create now 4 decisions how the story could continue.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb08c45ab9a9be3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T12:50:27.773891800Z"
    }
   },
   "outputs": [],
   "source": [
    "decision = \"c\"\n",
    "print(conversation_buf(f\"Write now the next chapter with using decision {decision}. Find a happy or sad ending based on the previous decisions.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a7d393b2f2f1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T12:50:27.773891800Z"
    }
   },
   "outputs": [],
   "source": [
    "print(conversation_buf(f\"Write now a detailed text to evaluate the decisions of the {MAIN_NAME}, and show what was good or bad and why! Please call {MAIN_NAME} by you.\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845ba48eb57e1ab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T12:50:27.773891800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b262d52217133f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T12:50:27.773891800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
